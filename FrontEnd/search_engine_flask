import json
from flask import Flask, request, jsonify
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re
from flask_cors import CORS
import time

app = Flask(__name__)
CORS(app)


def load_Lexicon():
    file_path = "Forward_Index/Lexicon.json"
    try:
        with open(file_path, "r") as file:
            return json.load(file)
    except (FileExistsError, FileNotFoundError):
        return None


def load_documentIndex():
    file_path = "Forward_Index/document_index.json"
    try:
        with open(file_path, "r") as file:
            return json.load(file)
    except (FileExistsError, FileNotFoundError):
        return None


def load_document_date_file():
    file_path = "Forward_Index/docId_date_mapping.json"
    try:
        with open(file_path, "r") as file:
            return json.load(file)
    except (FileExistsError, FileNotFoundError):
        return None


def load_inverted_index_barrel(path):
    try:
        with open(path, "r") as file:
            return json.load(file)
    except (FileExistsError, FileNotFoundError):
        return None


def get_document_ids(word_id, word_data_in_barrel):
    if str(word_id) in word_data_in_barrel["word_ID"]:
        word_data = word_data_in_barrel["word_ID"][str(word_id)]
        return word_data
    else:
        return {}


@app.route('/search', methods=['POST'])
def search_query():
    print("API CALLED")
    data = request.get_json()
    query = data.get('query')
    start_time = time.time()  # Record start time


    stop_words = set(stopwords.words('english'))

    lemmatizer = WordNetLemmatizer()

    inverted_index_file_paths = []
    for i in range(1, 101):
        inverted_index_file_paths.append(
            f'./Inverted_Index/Inverted_index_files/inverted_index_barrel_{i}.json')

    lexicon_dictionary = load_Lexicon()

    document_urls = load_documentIndex()

    loaded_inverted_indices = {}

    query_tokenized = word_tokenize(query)

    query_tokenized = [
        word for word in query_tokenized if re.match("^[a-zA-Z0-9_]*$", word)]

    query_tokenized = [word.lower() for word in query_tokenized]

    clean_query = [word for word in query_tokenized if word not in stop_words]

    clean_query = [lemmatizer.lemmatize(word) for word in clean_query]

    for word in clean_query:
        word_id = lexicon_dictionary[word]
        print(word_id)
        barrel_id = word_id % 100
        print(barrel_id + 1)

        if barrel_id in loaded_inverted_indices:
            word_data_in_barrel = loaded_inverted_indices[barrel_id]
        else:

            word_data_in_barrel = load_inverted_index_barrel(
                f'./Inverted_Index/Inverted_index_files/inverted_index_barrel_{barrel_id + 1}.json')
            loaded_inverted_indices[barrel_id] = word_data_in_barrel

        documents = get_document_ids(word_id, word_data_in_barrel)

        scores = {}
        for document_id, data in documents.items():
            frequency = data.get("fr", 0)
            positions = data.get("ps", [])

            position_score = sum(
                1 / pos for pos in positions if pos != 0) if positions else 0

            scores[document_id] = frequency * position_score

        sorted_documents = dict(
            sorted(scores.items(), key=lambda item: item[1], reverse=True)[:30])

        for document_id in sorted_documents.keys():
            document_url = document_urls[document_id]
            print(document_id, " ",
                  sorted_documents[document_id], " ", document_url)
    end_time = time.time()  # Record end time
    time_taken = end_time - start_time  # Calculate time taken

    response = {
        "message": "Received Query",
        "documents": sorted_documents,  # Modify this to include the actual search results
        "document_url": document_urls,  # Modify this to include the URLs for search results
        "time_taken": time_taken  # Include time taken in the response

    }

    return jsonify(response)


if __name__ == "__main__":
    app.run(debug=True)
